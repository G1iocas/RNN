{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN, BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract features from audio\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, res_type='kaiser_fast', duration=30) \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)\n",
    "        mfccs_processed = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_path)\n",
    "        return None \n",
    "     \n",
    "    return mfccs_processed\n",
    "\n",
    "# Define function to load data and extract features\n",
    "def load_data(data_path):\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    for folder in os.listdir(data_path):\n",
    "        genre_folder = os.path.join(data_path, folder)\n",
    "        for file in os.listdir(genre_folder):\n",
    "            file_path = os.path.join(genre_folder, file)\n",
    "            feature = extract_features(file_path)\n",
    "            if feature is not None:\n",
    "                features.append(feature)\n",
    "                labels.append(folder)\n",
    "                \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = 'Data/genres_original'  # Update with your dataset path\n",
    "features, labels = load_data(data_path)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_categorical, test_size=0.2, random_state=42)\n",
    "X_train.shape[0]\n",
    "# Reshape input data to have the appropriate shape for RNN\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_1\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(units_1, units_2, units_3, learning_rate=0.001, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=units_1, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))  # Add dropout layer\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(SimpleRNN(units=units_2, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))  # Add dropout layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=units_3))\n",
    "\n",
    "    model.add(Dropout(dropout_rate))  # Add dropout layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize variables to store best score and parameters\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "best_model = None\n",
    "\n",
    "# Define lists of hyperparameters to search over\n",
    "learning_rates = [0.001, 0.0015, 0.002]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "epochs_list = [30, 40, 50, 60]\n",
    "\n",
    "# Define lists of units for SimpleRNN layers\n",
    "units_list_1 = [64, 128, 256]\n",
    "units_list_2 = [32, 64, 128]\n",
    "units_list_3 = [16, 32, 64]\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epochs in epochs_list:\n",
    "                for units_1 in units_list_1:\n",
    "                    for units_2 in units_list_2:\n",
    "                        for units_3 in units_list_3:\n",
    "                            # Create model with current hyperparameters\n",
    "                            model = create_model(learning_rate=lr, dropout_rate=dropout_rate, units_1=units_1, units_2=units_2, units_3=units_3)\n",
    "                            \n",
    "                            # Train the model\n",
    "                            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "                            \n",
    "                            # Evaluate the model on validation data\n",
    "                            _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "                            \n",
    "                            # Store the accuracy and parameters\n",
    "                            params = {'learning_rate': lr, 'dropout_rate': dropout_rate, 'batch_size': batch_size, 'epochs': epochs, 'units_1': units_1, 'units_2': units_2, 'units_3': units_3}\n",
    "                            print(\"Accuracy:\", accuracy, \"Params:\", params)\n",
    "                            \n",
    "                            # Check if current model has the best score\n",
    "                            if accuracy > best_score:\n",
    "                                best_score = accuracy\n",
    "                                best_params = params\n",
    "                                best_model = model\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(\"Best Mean Test Accuracy:\", best_score)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a classification report for the best model\n",
    "y_pred = best_model.predict_classes(X_test)\n",
    "report = classification_report(np.argmax(y_test, axis=1), y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_2\n",
    "# Define a function to create the model\n",
    "def create_model(units_1, units_2, learning_rate=0.001, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=units_1, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))  # Add dropout layer\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(SimpleRNN(units=units_2))\n",
    "    model.add(Dropout(dropout_rate))  # Add dropout layer\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize variables to store best score and parameters\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "best_model = None\n",
    "\n",
    "# Define lists of hyperparameters to search over\n",
    "learning_rates = [0.001, 0.0015, 0.002]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "epochs_list = [30, 40, 50, 60]\n",
    "\n",
    "# Define lists of units for SimpleRNN layers\n",
    "units_list_1 = [64, 128, 256]\n",
    "units_list_2 = [32, 64, 128]\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epochs in epochs_list:\n",
    "                for units_1 in units_list_1:\n",
    "                    for units_2 in units_list_2:\n",
    "                        # Create model with current hyperparameters\n",
    "                        model = create_model(learning_rate=lr, dropout_rate=dropout_rate, units_1=units_1, units_2=units_2)\n",
    "                        \n",
    "                        # Train the model\n",
    "                        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "                        \n",
    "                        # Evaluate the model on validation data\n",
    "                        _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "                        \n",
    "                        # Store the accuracy and parameters\n",
    "                        params = {'learning_rate': lr, 'dropout_rate': dropout_rate, 'batch_size': batch_size, 'epochs': epochs, 'units_1': units_1, 'units_2': units_2}\n",
    "                        print(\"Accuracy:\", accuracy, \"Params:\", params)\n",
    "                        \n",
    "                        # Check if current model has the best score\n",
    "                        if accuracy > best_score:\n",
    "                            best_score = accuracy\n",
    "                            best_params = params\n",
    "                            best_model = model\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(\"Best Mean Test Accuracy:\", best_score)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a classification report for the best model\n",
    "y_pred = best_model.predict_classes(X_test)\n",
    "report = classification_report(np.argmax(y_test, axis=1), y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_3\n",
    "# Define a function to create the model\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.2, units=128):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=units, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "    model.add(Dropout(dropout_rate))  # Adjust dropout rate\n",
    "    model.add(BatchNormalization())  # Add BatchNormalization layer\n",
    "    model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize variables to store best score and parameters\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "best_model = None\n",
    "\n",
    "# Define lists of hyperparameters to search over\n",
    "learning_rates = [0.001, 0.0015, 0.002]\n",
    "dropout_rates = [0.1, 0.2, 0.3, 0.5]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "epochs_list = [30, 40, 50, 60]\n",
    "units_list = [64, 128, 256]\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for lr in learning_rates:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epochs in epochs_list:\n",
    "                for units in units_list:\n",
    "                    # Create model with current hyperparameters\n",
    "                    model = create_model(learning_rate=lr, dropout_rate=dropout_rate, units=units)\n",
    "                    \n",
    "                    # Train the model\n",
    "                    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "                    \n",
    "                    # Evaluate the model on validation data\n",
    "                    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "                    \n",
    "                    # Store the accuracy and parameters\n",
    "                    params = {'learning_rate': lr, 'dropout_rate': dropout_rate, 'batch_size': batch_size, 'epochs': epochs, 'units': units}\n",
    "                    print(\"Accuracy:\", accuracy, \"Params:\", params)\n",
    "                    \n",
    "                    # Check if current model has the best score\n",
    "                    if accuracy > best_score:\n",
    "                        best_score = accuracy\n",
    "                        best_params = params\n",
    "                        best_model = model\n",
    "\n",
    "# Print the best score and parameters\n",
    "print(\"Best Mean Test Accuracy:\", best_score)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Generate a classification report for the best model\n",
    "y_pred = best_model.predict_classes(X_test)\n",
    "report = classification_report(np.argmax(y_test, axis=1), y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=128, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "model.add(Dropout(0.1))  # Adjust dropout rate\n",
    "model.add(BatchNormalization())  # Add BatchNormalization layer\n",
    "model.add(Dense(units=len(label_encoder.classes_), activation='softmax'))\n",
    "    \n",
    "model.compile(optimizer=Adam(learning_rate=0.0015), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=128, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape test data\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# Step 1: Obtain predictions on the test data\n",
    "y_pred_prob = model.predict(X_test_reshaped)\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Step 2: Generate a classification report\n",
    "report = classification_report(np.argmax(y_test, axis=1), y_pred)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Save the model\n",
    "#model.save('music_genre_classifier_rnn.keras')\n",
    "#print(\"Model saved as 'music_genre_classifier_rnn.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
